[CRAB]

jobtype = cmssw
#use_server =  1
use_server =  0
scheduler = remoteGlidein
# scheduler = glite
#scheduler = condor 

###### TARKISTA SEURAAVAT ASIAT #####
# 1. datasetpath on oikea
# 2. datasetpathissa maariteltya datasettia vastaa oikea JSON
# 3. data tallentuu haluttuun kansioon
# 4. output tiedoston nimi on oikea
# 5. output tiedoston nimi on sama kuin JPsiPhiPAT_data_38X.py tiedostossa
# 6. JPsiPhiPAT_data_38X.py tiedostossa maaritelty global tag on datasetin global tag
# 7.  Mika analyysikoodin overlap check on kaytossa (Bd, Bp vai Bs overlap check!!) 
[CMSSW]

## ----------  run on data -----------------------------------
# ---------- Onia skimmed ------ find dataset where dataset like Onia*Jun14
# reprocessed runs
#datasetpath = /MinimumBias/Commissioning10-CS_Onia-Jun14thSkim_v1/Raw-RECO
#datasetpath = /Mu/Run2010A-CS_Onia-Jun14thSkim_v1/RAW-RECO

# prompt reco runs
#datasetpath = /Mu/Run2010A-CS_Onia-v6/RAW-RECO 
#datasetpath = /MuOnia/Run2010A-CS_Onia-v6/RAW-RECO
#datasetpath = /MuOnia/Run2010A-PromptReco-v4/RECO
#datasetpath = /MuOnia/Run2010B-PromptReco-v2/RECO
#datasetpath = /MuOnia/Run2010A-Sep17ReReco_v2/RECO
#datasetpath = /MuOnia/Run2010A-Nov4ReReco_v1/RECO
#datasetpath = /MuOnia/Run2010B-Nov4ReReco_v1/RECO 
#datasetpath = /MuOnia/Run2011A-PromptReco-v1/AOD
#datasetpath = /MuOnia/Run2011A-May10ReReco-v1/AOD
#datasetpath = /MuOnia/Run2011A-PromptReco-v2/AOD
#datasetpath = /MuOnia/Run2011A-PromptReco-v4/AOD
#datasetpath = /MuOnia/Run2011A-PromptReco-v5/AOD
#datasetpath = /MuOnia/Run2011A-PromptReco-v6/AOD
#datasetpath = /MuOnia/Run2011B-PromptReco-v1/AOD

#Terhin datat
datasetpath = /MuOnia/Run2012A-13Jul2012-v1/AOD
#datasetpath = /MuOnia/Run2012C-24Aug2012-v1/AOD 
#datasetpath = /MuOnia/Run2012C-PromptReco-v2/AOD

#datasetpath = /MuOnia/Run2012A-22Jan2013-v1/AOD  #run number 190456 193621 # 0.937320 fb-1 
#datasetpath = /MuOnia/Run2012B-22Jan2013-v1/AOD  #run number 193834 196531 # 4.828 fb-1
#datasetpath = /MuOnia/Run2012C-22Jan2013-v1/AOD  #run number 198022 203742 # 7.308 fb-1
#datasetpath = /MuOnia/Run2012D-22Jan2013-v1/AOD  #run number 203777 208686 # 7.032 fb-1


### The ParameterSet you want to use
pset = JPsiPhiPAT_data_38X.py
# pset = jpsiphi_data_cfg.py

# JSON for /MuOnia/Run2012A-13Jul2012-v1/AOD :
lumi_mask = /afs/cern.ch/user/t/terhi/scratch0/CMSSW_5_3_3_patch1/src/HeavyFlavorAnalysis/BsToJpsiPhi/test/Cert_190456-196531_8TeV_13Jul2012ReReco_Collisions12_JSON_MuonPhys_v4.txt

#JSON for /MuOnia/Run2012C-24Aug2012-v1/AOD :
#lumi_mask = /afs/cern.ch/user/t/terhi/scratch0/CMSSW_5_3_3_patch1/src/HeavyFlavorAnalysis/BsToJpsiPhi/test/Cert_198022-198523_8TeV_24Aug2012ReReco_Collisions12_JSON_MuonPhys.txt

#JSON for /MuOnia/Run2012C-PromptReco-v2/AOD :
#lumi_mask = /afs/cern.ch/user/t/terhi/scratch0/CMSSW_5_3_3_patch1/src/HeavyFlavorAnalysis/BsToJpsiPhi/test/Cert_190456-208357_8TeV_PromptReco_Collisions12_JSON_MuonPhys.txt

#JSON for /MuOnia/Run2012A-D 22Jan2013-v1/AOD: (this JSON is also used with jets)
#Cert_190456-208686_8TeV_22Jan2013ReReco_Collisions12_JSON.txt

#JSON for /MuOnia/Run2012A-D 22Jan2013-v1/AOD: (this JSON is also used with MUONS ONLY)
#Cert_190456-208686_8TeV_22Jan2013ReReco_Collisions12_JSON_MuonPhys.txt  # 20.651 fb-1 

### A JSON file that describes which runs and lumis to process. CRAB will skip luminosity blocks not
### listed in the file. When using this setting, you must also use lumi-based splitting rather than
### event based splitting as shown below.
# on data with json file
# lumi_mask = jsonls.json
# lumi_mask = jsonls.txt

#################################################################################################
# ---------- /MinimumBias/Commissioning10*May6*GOODCOLL*
#  lumi_mask = /afs/cern.ch/user/a/azzolini/scratch0/CMSSW_3_5_6/src/HeavyFlavorAnalysis/Onia2MuMu/certification/7TeV/Collisions10/Reprocessing/Cert_132440-134725_7TeV_MinimumBias_May6ReReco_Collisions10_JSON_BPAG.txt
# ---------- /MinimumBias/Commissioning10-GOODCOLL-v9*
#  lumi_mask = /afs/cern.ch/user/a/azzolini/scratch0/CMSSW_3_5_6/src/HeavyFlavorAnalysis/Onia2MuMu/certification/7TeV/Collisions10/StreamExpress/135059-136297_Stream_Collisions10_JSON_BPAG.txt
#################################################################################################

#################################################################################################
# ---------- Onia skimmed ------ find dataset where dataset like /MinimumBias/Commissioning10*May6*Onia
###-- reprocessed runs-----
### datasetpath:/MinimumBias/Commissioning10-CS_Onia-Jun14thSkim_v1/Raw-RECO(Run range:131511-135802) 
### datasetpath:/Mu/Run2010A-CS_Onia-Jun14thSkim_v1/RAW-RECO                (Run range:135821-137436)
# lumi_mask = /afs/cern.ch/user/a/azzolini/scratch0/CMSSW_3_5_6/src/HeavyFlavorAnalysis/Onia2MuMu/certification/7TeV/Collisions10/Reprocessing/Cert_132440-137028_7TeV_June14thReReco_Collisions10_JSON_BPAG_v2.txt
#lumi_mask = /afs/cern.ch/user/g/gfedi/scratch0/CMSSW_3_6_1_patch3/src/HeavyFlavorAnalysis/BsToJpsiPhi/test/Cert_132440-137028_7TeV_June14thReReco_Collisions10_JSON_BPAG_v2.txt

###-- prompt reco runs-----
### datasetpath = /Mu/Run2010A-CS_Onia-v6/RAW-RECO (Run range:138509-142137)
#lumi_mask = /afs/cern.ch/user/a/azzolini/scratch0/CMSSW_3_5_6/src/HeavyFlavorAnalysis/Onia2MuMu/certification/7TeV/Collisions10/StreamExpress/Cert_132440-140399_7TeV_StreamExpress_Collisions10_JSON_BPAG.txt
#lumi_mask = /afs/cern.ch/user/g/gfedi/scratch0/CMSSW_3_6_1_patch3/src/HeavyFlavorAnalysis/BsToJpsiPhi/test/Cert_132440-137028_7TeV_June14thReReco_Collisions10_JSON_BPAG_v2_SPLITTEDwq.txt
#lumi_mask = /afs/cern.ch/user/g/gfedi/scratch0/CMSSW_3_6_1_patch3/src/HeavyFlavorAnalysis/BsToJpsiPhi/test/Cert_132440-148058_7TeV_StreamExpress_Collisions10_JSON_BPAG.txt
#lumi_mask = Cert_132440-149442_7TeV_StreamExpress_Collisions10_JSON_BPAG.txt
#lumi_mask = Cert_132440-149442_7TeV_StreamExpress_Collisions10_JSON_BPAG_v2.txt
#lumi_mask = Cert_136033-149442_7TeV_Nov4ReReco_Collisions10_JSON_BPAG.txt
#lumi_mask = Cert_132440-144114_7TeV_Sep17ReReco_Collisions10_JSON_BPAG.txt
#lumi_mask = /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions10/7TeV/StreamExpress/Cert_132440-148058_7TeV_StreamExpress_Collisions10_JSON.txt 
#lumi_mask = /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions11/7TeV/Prompt/Cert_160404-161312_7TeV_PromptReco_Collisions11_JSON_MuonPhys.txt
#lumi_mask = /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions11/7TeV/Prompt/Cert_160404-163369_7TeV_PromptReco_Collisions11_JSON_MuonPhys.txt
#lumi_mask = /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions11/7TeV/Prompt/Cert_160404-163757_7TeV_PromptReco_Collisions11_JSON_MuonPhys.txt
#lumi_mask = /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions11/7TeV/Prompt/Cert_160404-176309_7TeV_PromptReco_Collisions11_JSON_MuonPhys.txt
#lumi_mask = /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions11/7TeV/Prompt/Cert_160404-180252_7TeV_PromptReco_Collisions11_JSON_MuonPhys.txt
#################################################################################################




#use_parent = 1
#get_edm_output = 1


### Splitting parameters:
### Total number of events to be accessed: -1 means all
total_number_of_events = -1
#total_number_of_events = -1

### Number of events to be processed per job
# events_per_job = 15000

### Analogous to events, lumis are used to split up analysis datasets
total_number_of_lumis = -1
#lumis_per_job = 1

### Number of jobs
#number_of_jobs = 1
number_of_jobs = 250

### The output files produced by your application (comma separated list)
output_file = BdOscilReReco13Jul2012Av1.root
#output_file = BData13Jul2012Av1.root
#output_file = BData24Aug2012Cv1.root
[USER]

## to have back the job executable output into UI (return_data= 1)
return_data = 0
email=terhi.jarvinen@cern.ch

### COPY JOB OUTPUT INTO A SE ###
copy_data = 1

### if copy_data = 1 ###
storage_element = madhatter.csc.fi
storage_path = /srm/managerv2?SFN=/pnfs/csc.fi/data/cms/store/
#storage_element=srm-cms.cern.ch
#storage_path=/srm/managerv2?SFN=/castor/cern.ch
logdir =/tmp/terhi/log 

##copy_data = 1
##storage_element=T2_FI_HIP
##publish_data=1
##publish_data_name = MCatNLO 
##dbs_url_for_publication = https://cmsdbsprod.cern.ch:8443/cms_dbs_ph_analysis_02_writer/servlet/DBSServlet



#----------------------------------------------------------------------------------------------------------------------------                                                                                           
#user_remote_dir = /user/terhi/Bdata13Jul2012
#user_remote_dir = /user/terhi/Bdata24Aug2012
user_remote_dir = /user/terhi/BdReReco13Jul2012A
#user_remote_dir = /user/t/terhi/testikansio
#----------------------------------------------------------------------------------------------------------------------------



## IMPORTANT create the dir in castor (e.g.)
##           add the permission to it or all the jobs will crash :-)
#rfmkdir /castor/cern.ch/user/u/username/subdir 
#rfchmod +775 /castor/cern.ch/user/u/username/subdir

Check_user_remote_dir = 0



#################################
####### JOB MONITORING  ### #####
#################################

### Use central BOSS DB instead of one for each task: the DB must be already been setup!
use_central_bossDB = 0

### Use Boss RealTime monitoring
use_boss_rt = 1 




[GRID]
################################
###### EDG specific stuff ######
################################

# LCG middleware version installed on testbed
lcg_version = 2


#remove_default_blacklist=1

## to change the CMS-broker RB. The ones available for CMS are "CERN" and "CNAF": the configuration
## files needed to change the broker will be automatically downloaded from CRAB web page. If the
## files are already present on the working directory they will be used. 
rb = CERN

## CMS myproxy server, to proxy delegation
proxy_server = myproxy.cern.ch 
dont_check_myproxy = 1
## Role in VOMS
#role = superman

## Group in VOMS
#group = superheros

## If you don't want CRAB to check your proxy
#dont_check_proxy = 1

## to add other requirements to jdl file, as example the Operating System
#requirements = (other.GlueHostOperatingSystemName == "RedHat")

## to add other parameters to jdl file: comma separated list, each item _must_
## be complete, including the closing ";"
additional_jdl_parameters = AllowZippedISB = false;

## cpu time and wall_clock_time(=real time) in minutes. Written into the jdl file
#max_cpu_time = 60
#max_wall_clock_time = 60

## SE Black List: all the storage elements (SE) containing the following strings (comma
## separated list) will not be considered for submission.
## for discovery, please use http://cmslcgco01.cern.ch:8001/
#se_black_list = helsinki.fi 
#se_black_list = dgc-grid-50.brunel.ac.uk, ppgrid1.rhul.ac.uk, t2-srm-02.lnl.infn.it, cmsdcache.pi.infn.it, lcg02.ciemat.es, lcg002.ihep.ac.cn
#ce_black_list = T2_IT_Pisa
#se_black_list = ifca.es 

## SE White List: only the storage elements (SE) containing the following strings (comma
## separated list) will be considered for submission.
## for discovery, please use http://cmslcgco01.cern.ch:8001/
#se_white_list = infn,cnaf,cscs,polgrid,desy,fzk,aachen,warsaw,T2_CN_Beijing
#se_white_list = T2_US_Purdue

## CE Black List: all the CE whose name contains the following strings (comma
## separated list) will not be considered for submission.
## Use the dns domain (eg fnal, cern, ifae, fzk, cnaf, lnl,....)
#ce_black_list = edu
#ce_black_list = ametisti


## CE White List: only the CE whose name contains the following strings (comma
## separated list) will be considered for submission.
## Use the dns domain (eg fnal, cern, ifae, fzk, cnaf, lnl,....)
#ce_white_list = T2_FI_HIP
#ce_white_list = infn,cnaf,cscs,polgrid,desy,fzk,aachen,warsaw,T2_CN_Beijing


## fields written into jdl
virtual_organization = cms

## number or retry count
retry_count = 4

## LFC catalog parameters
lcg_catalog_type = lfc
lfc_host = lfc-cms-test.cern.ch
lfc_home = /grid/cms

[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))

